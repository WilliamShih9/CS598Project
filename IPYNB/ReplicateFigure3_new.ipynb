{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "import os\n",
        "print(os.getcwd())\n",
        "os.chdir(\"/content/gdrive/My Drive/Colab Notebooks/NewVersion\")\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLpGLB2EI-IA",
        "outputId": "ffc04351-fa4a-4093-ef2d-9c3c2b093f3d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content\n",
            "/content/gdrive/My Drive/Colab Notebooks/NewVersion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "ZEBzIJlWSL9l"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "torch.manual_seed(1)\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "import copy\n",
        "\n",
        "##########################################################\n",
        "\n",
        "label_to_ix=np.load('label_to_ix_new.npy', allow_pickle = True).item()\n",
        "ix_to_label=np.load('ix_to_label_new.npy', allow_pickle = True)\n",
        "training_data=np.load('training_data_new.npy', allow_pickle = True)\n",
        "test_data=np.load('test_data_new.npy', allow_pickle = True)\n",
        "val_data=np.load('val_data_new.npy', allow_pickle = True)\n",
        "word_to_ix=np.load('word_to_ix_new.npy', allow_pickle = True).item()\n",
        "ix_to_word=np.load('ix_to_word_new.npy', allow_pickle = True)\n",
        "newwikivec=np.load('newwikivec_new.npy', allow_pickle = True)\n",
        "wikivoc=np.load('wikivoc_new.npy', allow_pickle = True).item()\n",
        "\n",
        "\n",
        "\n",
        "wikisize=newwikivec.shape[0]\n",
        "rvocsize=newwikivec.shape[1]\n",
        "wikivec=autograd.Variable(torch.FloatTensor(newwikivec))\n",
        "\n",
        "batchsize=32\n",
        "\n",
        "\n",
        "def preprocessing(data):\n",
        "\n",
        "    new_data=[]\n",
        "    for i, note, j in data:\n",
        "        templabel=[0.0]*len(label_to_ix)\n",
        "        for jj in j:\n",
        "            if jj in wikivoc:\n",
        "                templabel[label_to_ix[jj]]=1.0\n",
        "        templabel=np.array(templabel,dtype=float)\n",
        "        new_data.append((i, note, templabel))\n",
        "    new_data=np.array(new_data)\n",
        "    \n",
        "    lenlist=[]\n",
        "    for i in new_data:\n",
        "        lenlist.append(len(i[0]))\n",
        "    sortlen=sorted(range(len(lenlist)), key=lambda k: lenlist[k])  \n",
        "    new_data=new_data[sortlen]\n",
        "    \n",
        "    batch_data=[]\n",
        "    \n",
        "    for start_ix in range(0, len(new_data)-batchsize+1, batchsize):\n",
        "        thisblock=new_data[start_ix:start_ix+batchsize]\n",
        "        mybsize= len(thisblock)\n",
        "        numword=np.max([len(ii[0]) for ii in thisblock])\n",
        "        main_matrix = np.zeros((mybsize, numword), dtype= np.int)\n",
        "        for i in range(main_matrix.shape[0]):\n",
        "            for j in range(main_matrix.shape[1]):\n",
        "                try:\n",
        "                    if thisblock[i][0][j] in word_to_ix:\n",
        "                        main_matrix[i,j] = word_to_ix[thisblock[i][0][j]]\n",
        "                    \n",
        "                except IndexError:\n",
        "                    pass       # because initialze with 0, so you pad with 0\n",
        "    \n",
        "        xxx2=[]\n",
        "        yyy=[]\n",
        "        for ii in thisblock:\n",
        "            xxx2.append(ii[1])\n",
        "            yyy.append(ii[2])\n",
        "        \n",
        "        xxx2=np.array(xxx2)\n",
        "        yyy=np.array(yyy)\n",
        "        batch_data.append((autograd.Variable(torch.from_numpy(main_matrix)),autograd.Variable(torch.FloatTensor(xxx2)),autograd.Variable(torch.FloatTensor(yyy))))\n",
        "    return batch_data\n",
        "batchtraining_data=preprocessing(training_data)\n",
        "batchtest_data=preprocessing(test_data)\n",
        "batchval_data=preprocessing(val_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKnldb4zIIP1",
        "outputId": "faad22ab-d064-4e03-8780-318869d28654"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-217e38864994>:42: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  new_data=np.array(new_data)\n",
            "<ipython-input-3-217e38864994>:56: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  main_matrix = np.zeros((mybsize, numword), dtype= np.int)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "######################################################################\n",
        "# Create the model:\n",
        "\n",
        "Embeddingsize=100\n",
        "hidden_dim=200\n",
        "class CNN(nn.Module):\n",
        "\n",
        "    def __init__(self, batch_size, vocab_size, tagset_size):\n",
        "        super(CNN, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size+1, Embeddingsize, padding_idx=0)\n",
        "        self.embed_drop = nn.Dropout(p=0.2)\n",
        "        \n",
        "        self.hidden2tag = nn.Linear(300, tagset_size)\n",
        "        \n",
        "        \n",
        "        self.convs1 = nn.Conv1d(Embeddingsize,100,3)\n",
        "        self.convs2 = nn.Conv1d(Embeddingsize,100,4)\n",
        "        self.convs3 = nn.Conv1d(Embeddingsize,100,5)\n",
        "        \n",
        "        \n",
        "        self.layer2 = nn.Linear(Embeddingsize, 1,bias=False)\n",
        "        self.embedding=nn.Linear(rvocsize,Embeddingsize)\n",
        "        self.vattention=nn.Linear(Embeddingsize,Embeddingsize)\n",
        "        \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "    \n",
        "    def forward(self, vec1, nvec, wiki, simlearning):\n",
        "       \n",
        "        thisembeddings=self.word_embeddings(vec1)\n",
        "        thisembeddings = self.embed_drop(thisembeddings)\n",
        "        thisembeddings=thisembeddings.transpose(1,2)\n",
        "        \n",
        "        output1=self.tanh(self.convs1(thisembeddings))\n",
        "        output1=nn.MaxPool1d(output1.size()[2])(output1)\n",
        "        \n",
        "        output2=self.tanh(self.convs2(thisembeddings))\n",
        "        output2=nn.MaxPool1d(output2.size()[2])(output2)\n",
        "        \n",
        "        output3=self.tanh(self.convs3(thisembeddings))\n",
        "        output3=nn.MaxPool1d(output3.size()[2])(output3)\n",
        "        \n",
        "        output4 = torch.cat([output1,output2,output3], 1).squeeze(2)\n",
        "        \n",
        "        if simlearning==1:\n",
        "            nvec=nvec.view(batchsize,1,-1)\n",
        "            nvec=nvec.expand(batchsize,wiki.size()[0],-1)\n",
        "            wiki=wiki.view(1,wiki.size()[0],-1)\n",
        "            wiki=wiki.expand(nvec.size()[0],wiki.size()[1],-1)\n",
        "            new=wiki*nvec\n",
        "            new=self.embedding(new)\n",
        "            vattention=self.sigmoid(self.vattention(new))\n",
        "            new=new*vattention\n",
        "            vec3=self.layer2(new)\n",
        "            vec3=vec3.view(batchsize,-1)\n",
        "        \n",
        "       \n",
        "        vec2 = self.hidden2tag(output4)\n",
        "        if simlearning==1:\n",
        "            tag_scores = self.sigmoid(vec2.detach()+vec3)\n",
        "        else:\n",
        "            tag_scores = self.sigmoid(vec2)\n",
        "        \n",
        "        \n",
        "        return tag_scores"
      ],
      "metadata": {
        "id": "0v2cLfdgIPDC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "# Create the model:\n",
        "\n",
        "Embeddingsize=100\n",
        "hidden_dim=200\n",
        "class CAML(nn.Module):\n",
        "\n",
        "    def __init__(self, batch_size, vocab_size, tagset_size):\n",
        "        super(CAML, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size+1, Embeddingsize, padding_idx=0)\n",
        "        self.embed_drop = nn.Dropout(p=0.2)   \n",
        "        \n",
        "        \n",
        "        self.convs1 = nn.Conv1d(Embeddingsize,300,10,padding=5)\n",
        "        self.H=nn.Linear(300, tagset_size )   \n",
        "        self.final = nn.Linear(300, tagset_size)\n",
        "        \n",
        "        self.layer2 = nn.Linear(Embeddingsize, 1)\n",
        "        self.embedding=nn.Linear(rvocsize,Embeddingsize,bias=False)\n",
        "        self.vattention=nn.Linear(Embeddingsize,Embeddingsize)\n",
        "        \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "    \n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "    \n",
        "    def forward(self, vec1, nvec, wiki, simlearning):\n",
        "        \n",
        "       \n",
        "        thisembeddings=self.word_embeddings(vec1)\n",
        "        thisembeddings = self.embed_drop(thisembeddings)\n",
        "        thisembeddings=thisembeddings.transpose(1,2)\n",
        "        \n",
        "        \n",
        "        thisembeddings=self.tanh(self.convs1(thisembeddings).transpose(1,2))  \n",
        "        \n",
        "        alpha=self.H.weight.matmul(thisembeddings.transpose(1,2))\n",
        "        alpha=F.softmax(alpha, dim=2)\n",
        "        \n",
        "        m=alpha.matmul(thisembeddings)\n",
        "       \n",
        "        myfinal=self.final.weight.mul(m).sum(dim=2).add(self.final.bias)\n",
        "        \n",
        "        if simlearning==1:\n",
        "            nvec=nvec.view(batchsize,1,-1)\n",
        "            nvec=nvec.expand(batchsize,wiki.size()[0],-1)\n",
        "            wiki=wiki.view(1,wiki.size()[0],-1)\n",
        "            wiki=wiki.expand(nvec.size()[0],wiki.size()[1],-1)\n",
        "            new=wiki*nvec\n",
        "            new=self.embedding(new)\n",
        "            vattention=self.sigmoid(self.vattention(new))\n",
        "            new=new*vattention\n",
        "            vec3=self.layer2(new)\n",
        "            vec3=vec3.view(batchsize,-1)\n",
        "        \n",
        "       \n",
        "        if simlearning==1:\n",
        "            tag_scores = self.sigmoid(myfinal.detach()+vec3)\n",
        "        else:\n",
        "            tag_scores = self.sigmoid(myfinal)\n",
        "        \n",
        "        \n",
        "        return tag_scores\n",
        "\n",
        "\n",
        "\n",
        "topk=10"
      ],
      "metadata": {
        "id": "hkav25yXIT-W"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "# Create the model:\n",
        "\n",
        "Embeddingsize=100\n",
        "hidden_dim=200\n",
        "class LSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, batch_size, vocab_size, tagset_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size+1, Embeddingsize, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(Embeddingsize, hidden_dim)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "        self.hidden = self.init_hidden()\n",
        "        \n",
        "        \n",
        "        self.layer2 = nn.Linear(Embeddingsize, 1,bias=False)\n",
        "        self.embedding=nn.Linear(rvocsize,Embeddingsize)\n",
        "        self.vattention=nn.Linear(Embeddingsize,Embeddingsize,bias=False)\n",
        "        \n",
        "        self.softmax = nn.Softmax()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.embed_drop = nn.Dropout(p=0.2)\n",
        "    \n",
        "    def init_hidden(self):\n",
        "        return (autograd.Variable(torch.zeros(1, batchsize, self.hidden_dim).cuda()),\n",
        "                autograd.Variable(torch.zeros(1, batchsize, self.hidden_dim)).cuda())\n",
        "\n",
        "    \n",
        "    def forward(self, vec1, nvec, wiki, simlearning):\n",
        "      \n",
        "        thisembeddings=self.word_embeddings(vec1).transpose(0,1)\n",
        "        thisembeddings = self.embed_drop(thisembeddings)\n",
        "       \n",
        "        if simlearning==1:\n",
        "            nvec=nvec.view(batchsize,1,-1)\n",
        "            nvec=nvec.expand(batchsize,wiki.size()[0],-1)\n",
        "            wiki=wiki.view(1,wiki.size()[0],-1)\n",
        "            wiki=wiki.expand(nvec.size()[0],wiki.size()[1],-1)\n",
        "            new=wiki*nvec\n",
        "            new=self.embedding(new)\n",
        "            vattention=self.sigmoid(self.vattention(new))\n",
        "            new=new*vattention\n",
        "            vec3=self.layer2(new)\n",
        "            vec3=vec3.view(batchsize,-1)\n",
        "        \n",
        "        \n",
        "        \n",
        "        lstm_out, self.hidden = self.lstm(\n",
        "            thisembeddings, self.hidden)\n",
        "        \n",
        "        lstm_out=lstm_out.transpose(0,2).transpose(0,1)\n",
        "        \n",
        "        output1=nn.MaxPool1d(lstm_out.size()[2])(lstm_out).view(batchsize,-1)\n",
        "        \n",
        "        vec2 = self.hidden2tag(output1)\n",
        "        if simlearning==1:\n",
        "            tag_scores = self.sigmoid(vec2.detach()+vec3)\n",
        "        else:\n",
        "            tag_scores = self.sigmoid(vec2)\n",
        "        \n",
        "        \n",
        "        return tag_scores"
      ],
      "metadata": {
        "id": "faXeGrtjIf3L"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "######################################################################\n",
        "# Create the model:\n",
        "\n",
        "Embeddingsize=100\n",
        "hidden_dim=200\n",
        "class LSTMattn(nn.Module):\n",
        "\n",
        "    def __init__(self, batch_size, vocab_size, tagset_size):\n",
        "        super(LSTMattn, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size+1, Embeddingsize, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(Embeddingsize, hidden_dim)\n",
        "        self.hidden = self.init_hidden()\n",
        "        \n",
        "        self.H=nn.Linear(hidden_dim, tagset_size )  \n",
        "        self.final = nn.Linear(hidden_dim, tagset_size)\n",
        "        \n",
        "        self.layer2 = nn.Linear(Embeddingsize, 1,bias=False)\n",
        "        self.embedding=nn.Linear(rvocsize,Embeddingsize)\n",
        "        self.vattention=nn.Linear(Embeddingsize,Embeddingsize,bias=False)\n",
        "        \n",
        "        self.softmax = nn.Softmax()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.embed_drop = nn.Dropout(p=0.2)\n",
        "    \n",
        "    def init_hidden(self):\n",
        "        return (autograd.Variable(torch.zeros(1, batchsize, self.hidden_dim).cuda()),\n",
        "                autograd.Variable(torch.zeros(1, batchsize, self.hidden_dim)).cuda())\n",
        "\n",
        "    \n",
        "    def forward(self, vec1, nvec, wiki, simlearning):\n",
        "        \n",
        "        \n",
        "        thisembeddings=self.word_embeddings(vec1).transpose(0,1)\n",
        "        thisembeddings = self.embed_drop(thisembeddings)\n",
        "        \n",
        "        \n",
        "        if simlearning==1:\n",
        "            nvec=nvec.view(batchsize,1,-1)\n",
        "            nvec=nvec.expand(batchsize,wiki.size()[0],-1)\n",
        "            wiki=wiki.view(1,wiki.size()[0],-1)\n",
        "            wiki=wiki.expand(nvec.size()[0],wiki.size()[1],-1)\n",
        "            new=wiki*nvec\n",
        "            new=self.embedding(new)\n",
        "            vattention=self.sigmoid(self.vattention(new))\n",
        "            new=new*vattention\n",
        "            vec3=self.layer2(new)\n",
        "            vec3=vec3.view(batchsize,-1)\n",
        "        \n",
        "        \n",
        "        lstm_out, self.hidden = self.lstm(\n",
        "            thisembeddings, self.hidden)\n",
        "        \n",
        "        \n",
        "        \n",
        "        lstm_out=lstm_out.transpose(0,1)\n",
        "\n",
        "        alpha=self.H.weight.matmul(lstm_out.transpose(1,2))\n",
        "        alpha=F.softmax(alpha, dim=2)\n",
        "        \n",
        "        m=alpha.matmul(lstm_out)\n",
        "        \n",
        "        myfinal=self.final.weight.mul(m).sum(dim=2).add(self.final.bias)\n",
        "        \n",
        "        \n",
        "        if simlearning==1:\n",
        "            tag_scores = self.sigmoid(myfinal.detach()+vec3)\n",
        "        else:\n",
        "            tag_scores = self.sigmoid(myfinal)\n",
        "        \n",
        "        \n",
        "        return tag_scores"
      ],
      "metadata": {
        "id": "-c7yX8XQIgS6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXx48q1BHi-E",
        "outputId": "a3c8609d-1557-4724-bc69-b7931e7849b1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CAML(\n",
              "  (word_embeddings): Embedding(47962, 100, padding_idx=0)\n",
              "  (embed_drop): Dropout(p=0.2, inplace=False)\n",
              "  (convs1): Conv1d(100, 300, kernel_size=(10,), stride=(1,), padding=(5,))\n",
              "  (H): Linear(in_features=300, out_features=344, bias=True)\n",
              "  (final): Linear(in_features=300, out_features=344, bias=True)\n",
              "  (layer2): Linear(in_features=100, out_features=1, bias=True)\n",
              "  (embedding): Linear(in_features=12881, out_features=100, bias=False)\n",
              "  (vattention): Linear(in_features=100, out_features=100, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              "  (tanh): Tanh()\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "CAML_model = CAML(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "CAML_model.load_state_dict(torch.load(\"CAML_model_new\"))\n",
        "CAML_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CNN_model = CNN(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "CNN_model.load_state_dict(torch.load(\"CNN_model_new\"))\n",
        "CNN_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvCNJBZ1Js6H",
        "outputId": "b13b0c71-bfe9-4515-eae8-d5f124d2e1ec"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (word_embeddings): Embedding(47962, 100, padding_idx=0)\n",
              "  (embed_drop): Dropout(p=0.2, inplace=False)\n",
              "  (hidden2tag): Linear(in_features=300, out_features=344, bias=True)\n",
              "  (convs1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
              "  (convs2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
              "  (convs3): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
              "  (layer2): Linear(in_features=100, out_features=1, bias=False)\n",
              "  (embedding): Linear(in_features=12881, out_features=100, bias=True)\n",
              "  (vattention): Linear(in_features=100, out_features=100, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              "  (tanh): Tanh()\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LSTM_model = LSTM(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "LSTM_model.load_state_dict(torch.load(\"LSTM_model_new\"))\n",
        "LSTM_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QiveNqvKMZ_",
        "outputId": "56176132-48f1-422b-ffa5-3ebf28f01db2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (word_embeddings): Embedding(47962, 100, padding_idx=0)\n",
              "  (lstm): LSTM(100, 200)\n",
              "  (hidden2tag): Linear(in_features=200, out_features=344, bias=True)\n",
              "  (layer2): Linear(in_features=100, out_features=1, bias=False)\n",
              "  (embedding): Linear(in_features=12881, out_features=100, bias=True)\n",
              "  (vattention): Linear(in_features=100, out_features=100, bias=False)\n",
              "  (softmax): Softmax(dim=None)\n",
              "  (sigmoid): Sigmoid()\n",
              "  (tanh): Tanh()\n",
              "  (embed_drop): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LSTMattn_model = LSTMattn(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "LSTMattn_model.load_state_dict(torch.load(\"LSTMattn_model_new\"))\n",
        "LSTMattn_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjZR1NFNKTLU",
        "outputId": "a6f3f90e-30b9-4dbf-fa76-0f91a1014105"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMattn(\n",
              "  (word_embeddings): Embedding(47962, 100, padding_idx=0)\n",
              "  (lstm): LSTM(100, 200)\n",
              "  (H): Linear(in_features=200, out_features=344, bias=True)\n",
              "  (final): Linear(in_features=200, out_features=344, bias=True)\n",
              "  (layer2): Linear(in_features=100, out_features=1, bias=False)\n",
              "  (embedding): Linear(in_features=12881, out_features=100, bias=True)\n",
              "  (vattention): Linear(in_features=100, out_features=100, bias=False)\n",
              "  (softmax): Softmax(dim=None)\n",
              "  (sigmoid): Sigmoid()\n",
              "  (embed_drop): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "KSI_CAML_model = CAML(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "KSI_CAML_model.load_state_dict(torch.load(\"KSI_CAML_model_new\"))\n",
        "KSI_CNN_model = CNN(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "KSI_CNN_model.load_state_dict(torch.load(\"KSI_CNN_model_new\"))\n",
        "KSI_LSTM_model = LSTM(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "KSI_LSTM_model.load_state_dict(torch.load(\"KSI_LSTM_model_new\"))\n",
        "KSI_LSTMattn_model = LSTMattn(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "KSI_LSTMattn_model.load_state_dict(torch.load(\"KSI_LSTMattn_model_new\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcmLK5DLKhKT",
        "outputId": "458ea34d-1397-4f6c-8502-6956b331bd50"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets = [batch[2] for batch in batchtest_data]\n",
        "targets_train = [batch[2] for batch in batchtraining_data]\n",
        "targets_val = [batch[2] for batch in batchval_data]\n",
        "targets.extend(targets_train)\n",
        "targets.extend(targets_val)"
      ],
      "metadata": {
        "id": "0Wa-xo0BNere"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_freq = torch.stack(targets).sum(dim = 0).sum(dim = 0)"
      ],
      "metadata": {
        "id": "e_g8HzsvNvhS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_divide = [ [], [], [], [], []]"
      ],
      "metadata": {
        "id": "FG1VNYVWK4eR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label divide index 0 is the labels that occur in the data 1-10 times.\n",
        "\n",
        "Label divide index 1 is the labels that occur in the data 11-50 times.\n",
        "\n",
        "Label divide index 2 is the labels that occur in the data 51-100 times.\n",
        "\n",
        "Label divide index 3 is the labels that occur in the data 100-500 times.\n",
        "\n",
        "Label divide index 4 is the labels that occur in the data 501+ times."
      ],
      "metadata": {
        "id": "uRfaWSg9Q8W0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for count, label in enumerate(label_freq):\n",
        "  if label > 500:\n",
        "    label_divide[4].append(count)\n",
        "  elif label > 100:\n",
        "    label_divide[3].append(count)\n",
        "  elif label > 50:\n",
        "    label_divide[2].append(count)\n",
        "  elif label > 10:\n",
        "    label_divide[1].append(count)\n",
        "  else:\n",
        "    label_divide[0].append(count)"
      ],
      "metadata": {
        "id": "oGCmQS0sP2Wh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[len(labels) for labels in label_divide]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CMEfgMGQphd",
        "outputId": "5f489c3f-33bd-40c6-b2de-a1a534fa6c94"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[80, 73, 25, 82, 84]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def testmodel(model, sim):\n",
        "    model.cuda()\n",
        "    loss_function = nn.BCELoss()\n",
        "    model.eval()\n",
        "    recall=[]\n",
        "    lossestest = []\n",
        "    \n",
        "    y_true=[]\n",
        "    y_scores=[]\n",
        "    \n",
        "    for inputs in batchtest_data:\n",
        "        with torch.no_grad():\n",
        "          targets = inputs[2].cuda()\n",
        "          \n",
        "          tag_scores = model(inputs[0].cuda(),inputs[1].cuda() ,wikivec.cuda(),sim)\n",
        "\n",
        "          loss = loss_function(tag_scores, targets)\n",
        "          \n",
        "          targets=targets.data.cpu().numpy()\n",
        "          tag_scores= tag_scores.data.cpu().numpy()\n",
        "          \n",
        "          \n",
        "          lossestest.append(loss.data.cpu().numpy().mean())\n",
        "          y_true.append(targets)\n",
        "          y_scores.append(tag_scores)\n",
        "        \n",
        "        for iii in range(0,len(tag_scores)):\n",
        "            temp={}\n",
        "            for iiii in range(0,len(tag_scores[iii])):\n",
        "                temp[iiii]=tag_scores[iii][iiii]\n",
        "            temp1=[(k, temp[k]) for k in sorted(temp, key=temp.get, reverse=True)]\n",
        "            thistop=int(np.sum(targets[iii]))\n",
        "            hit=0.0\n",
        "            \n",
        "            for ii in temp1[0:max(thistop,topk)]:\n",
        "                if targets[iii][ii[0]]==1.0:\n",
        "                    hit=hit+1\n",
        "            if thistop!=0:\n",
        "                recall.append(hit/thistop)\n",
        "    y_true=np.concatenate(y_true,axis=0)\n",
        "    y_scores=np.concatenate(y_scores,axis=0)\n",
        "    y_true=y_true.T\n",
        "    y_scores=y_scores.T\n",
        "    macro_AUC = []\n",
        "    for labels in label_divide:\n",
        "      y_true_temp = y_true[labels]\n",
        "      y_scores_temp = y_scores[labels]\n",
        "      temptrue=[]\n",
        "      tempscores=[]\n",
        "      for col in range(0,len(y_true_temp)):\n",
        "          if np.sum(y_true_temp[col])!=0:\n",
        "              temptrue.append(y_true_temp[col])\n",
        "              tempscores.append(y_scores_temp[col])\n",
        "      temptrue=np.array(temptrue)\n",
        "      tempscores=np.array(tempscores)\n",
        "      y_true_temp=temptrue.T\n",
        "      y_scores_temp=tempscores.T\n",
        "      macro_AUC.append(roc_auc_score(y_true_temp, y_scores_temp,average='macro'))\n",
        "    return macro_AUC"
      ],
      "metadata": {
        "id": "zOxfYCbsVi_A"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RNN_model_results = testmodel(LSTM_model, 0)\n",
        "KSI_RNN_model_results = testmodel(KSI_LSTM_model, 1)\n",
        "RNNattn_model_results = testmodel(LSTMattn_model, 0)\n",
        "KSI_RNNattn_model_results = testmodel(KSI_LSTMattn_model, 1)\n",
        "CNN_model_results = testmodel(CNN_model, 0)\n",
        "KSI_CNN_model_results = testmodel(KSI_CNN_model, 1)\n",
        "CAML_model_results = testmodel(CAML_model, 0)\n",
        "KSI_CAML_model_reults = testmodel(KSI_CAML_model, 1)"
      ],
      "metadata": {
        "id": "cwOlyAI1VORI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = np.array([RNN_model_results,\n",
        "KSI_RNN_model_results,\n",
        "RNNattn_model_results,\n",
        "KSI_RNNattn_model_results, \n",
        "CNN_model_results,\n",
        "KSI_CNN_model_results,\n",
        "CAML_model_results,\n",
        "KSI_CAML_model_reults])"
      ],
      "metadata": {
        "id": "fwxLhtAcZu7v"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "XK__86XBbEEL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = pd.DataFrame(results, index=[\"RNN\", \"KSI+RNN\", \"RNNatt\", \"KSI+RNNatt\", \"CNN\", \"KSI+CNN\", \"CAML\", \"KSI+CAML\"],\n",
        "                             columns=['1-10', '11-50','51-100','101-500','>500'])"
      ],
      "metadata": {
        "id": "x2aElfJrZ-nw"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result.round(4).to_latex())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiQZ6B31bzL4",
        "outputId": "c3b6e68b-3c65-4f14-e329-38aff24202bc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\begin{tabular}{lrrrrr}\n",
            "\\toprule\n",
            "{} &    1-10 &   11-50 &  51-100 &  101-500 &    >500 \\\\\n",
            "\\midrule\n",
            "RNN        &  0.7407 &  0.7565 &  0.8089 &   0.8444 &  0.9048 \\\\\n",
            "KSI+RNN    &  0.7945 &  0.8243 &  0.8846 &   0.8986 &  0.9219 \\\\\n",
            "RNNatt     &  0.6771 &  0.7894 &  0.8528 &   0.8928 &  0.9232 \\\\\n",
            "KSI+RNNatt &  0.7765 &  0.8459 &  0.8997 &   0.9231 &  0.9318 \\\\\n",
            "CNN        &  0.7277 &  0.7800 &  0.8158 &   0.8606 &  0.9090 \\\\\n",
            "KSI+CNN    &  0.7775 &  0.8297 &  0.8880 &   0.8981 &  0.9234 \\\\\n",
            "CAML       &  0.7267 &  0.7764 &  0.8744 &   0.9113 &  0.9337 \\\\\n",
            "KSI+CAML   &  0.7960 &  0.8507 &  0.9117 &   0.9274 &  0.9360 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-cb30f152a4d1>:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
            "  print(result.round(4).to_latex())\n"
          ]
        }
      ]
    }
  ]
}
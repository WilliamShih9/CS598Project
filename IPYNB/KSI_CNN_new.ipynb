{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-7Us8FNqpTq",
        "outputId": "0658fcb8-04db-4443-87d2-3904ed683023"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "os.chdir(\"/content/gdrive/My Drive/Colab Notebooks/NewVersion\")\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJhu2bNdqrTN",
        "outputId": "4715ffed-241a-448d-a2e9-72cb7fa4069f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/gdrive/My Drive/Colab Notebooks/NewVersion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Cq4raF18p8cS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "torch.manual_seed(1)\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "import copy\n",
        "\n",
        "##########################################################\n",
        "\n",
        "label_to_ix=np.load('label_to_ix_new.npy', allow_pickle = True).item()\n",
        "ix_to_label=np.load('ix_to_label_new.npy', allow_pickle = True)\n",
        "training_data=np.load('training_data_new.npy', allow_pickle = True)\n",
        "test_data=np.load('test_data_new.npy', allow_pickle = True)\n",
        "val_data=np.load('val_data_new.npy', allow_pickle = True)\n",
        "word_to_ix=np.load('word_to_ix_new.npy', allow_pickle = True).item()\n",
        "ix_to_word=np.load('ix_to_word_new.npy', allow_pickle = True)\n",
        "newwikivec=np.load('newwikivec_new.npy', allow_pickle = True)\n",
        "wikivoc=np.load('wikivoc_new.npy', allow_pickle = True).item()\n",
        "\n",
        "\n",
        "\n",
        "wikisize=newwikivec.shape[0]\n",
        "rvocsize=newwikivec.shape[1]\n",
        "wikivec=autograd.Variable(torch.FloatTensor(newwikivec))\n",
        "\n",
        "batchsize=32"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocessing(data):\n",
        "\n",
        "    new_data=[]\n",
        "    for i, note, j in data:\n",
        "        templabel=[0.0]*len(label_to_ix)\n",
        "        for jj in j:\n",
        "            if jj in wikivoc:\n",
        "                templabel[label_to_ix[jj]]=1.0\n",
        "        templabel=np.array(templabel,dtype=float)\n",
        "        new_data.append((i, note, templabel))\n",
        "    new_data=np.array(new_data)\n",
        "    \n",
        "    lenlist=[]\n",
        "    for i in new_data:\n",
        "        lenlist.append(len(i[0]))\n",
        "    sortlen=sorted(range(len(lenlist)), key=lambda k: lenlist[k])  \n",
        "    new_data=new_data[sortlen]\n",
        "    \n",
        "    batch_data=[]\n",
        "    \n",
        "    for start_ix in range(0, len(new_data)-batchsize+1, batchsize):\n",
        "        thisblock=new_data[start_ix:start_ix+batchsize]\n",
        "        mybsize= len(thisblock)\n",
        "        numword=np.max([len(ii[0]) for ii in thisblock])\n",
        "        main_matrix = np.zeros((mybsize, numword), dtype= np.int)\n",
        "        for i in range(main_matrix.shape[0]):\n",
        "            for j in range(main_matrix.shape[1]):\n",
        "                try:\n",
        "                    if thisblock[i][0][j] in word_to_ix:\n",
        "                        main_matrix[i,j] = word_to_ix[thisblock[i][0][j]]\n",
        "                    \n",
        "                except IndexError:\n",
        "                    pass       # because initialze with 0, so you pad with 0\n",
        "    \n",
        "        xxx2=[]\n",
        "        yyy=[]\n",
        "        for ii in thisblock:\n",
        "            xxx2.append(ii[1])\n",
        "            yyy.append(ii[2])\n",
        "        \n",
        "        xxx2=np.array(xxx2)\n",
        "        yyy=np.array(yyy)\n",
        "        batch_data.append((autograd.Variable(torch.from_numpy(main_matrix)),autograd.Variable(torch.FloatTensor(xxx2)),autograd.Variable(torch.FloatTensor(yyy))))\n",
        "    return batch_data\n",
        "batchtraining_data=preprocessing(training_data)\n",
        "batchtest_data=preprocessing(test_data)\n",
        "batchval_data=preprocessing(val_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtDJdZ1XqVKk",
        "outputId": "7f69d64c-7b71-4a95-a85d-8004c44a559f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-c51cf0410f4f>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  new_data=np.array(new_data)\n",
            "<ipython-input-4-c51cf0410f4f>:25: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  main_matrix = np.zeros((mybsize, numword), dtype= np.int)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Create the model:\n",
        "\n",
        "Embeddingsize=100\n",
        "hidden_dim=200\n",
        "class CNN(nn.Module):\n",
        "\n",
        "    def __init__(self, batch_size, vocab_size, tagset_size):\n",
        "        super(CNN, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size+1, Embeddingsize, padding_idx=0)\n",
        "        self.embed_drop = nn.Dropout(p=0.2)\n",
        "        \n",
        "        self.hidden2tag = nn.Linear(300, tagset_size)\n",
        "        \n",
        "        \n",
        "        self.convs1 = nn.Conv1d(Embeddingsize,100,3)\n",
        "        self.convs2 = nn.Conv1d(Embeddingsize,100,4)\n",
        "        self.convs3 = nn.Conv1d(Embeddingsize,100,5)\n",
        "        \n",
        "        \n",
        "        self.layer2 = nn.Linear(Embeddingsize, 1,bias=False)\n",
        "        self.embedding=nn.Linear(rvocsize,Embeddingsize)\n",
        "        self.vattention=nn.Linear(Embeddingsize,Embeddingsize)\n",
        "        \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "    \n",
        "    def forward(self, vec1, nvec, wiki, simlearning):\n",
        "       \n",
        "        thisembeddings=self.word_embeddings(vec1)\n",
        "        thisembeddings = self.embed_drop(thisembeddings)\n",
        "        thisembeddings=thisembeddings.transpose(1,2)\n",
        "        \n",
        "        output1=self.tanh(self.convs1(thisembeddings))\n",
        "        output1=nn.MaxPool1d(output1.size()[2])(output1)\n",
        "        \n",
        "        output2=self.tanh(self.convs2(thisembeddings))\n",
        "        output2=nn.MaxPool1d(output2.size()[2])(output2)\n",
        "        \n",
        "        output3=self.tanh(self.convs3(thisembeddings))\n",
        "        output3=nn.MaxPool1d(output3.size()[2])(output3)\n",
        "        \n",
        "        output4 = torch.cat([output1,output2,output3], 1).squeeze(2)\n",
        "        \n",
        "        if simlearning==1:\n",
        "            nvec=nvec.view(batchsize,1,-1)\n",
        "            nvec=nvec.expand(batchsize,wiki.size()[0],-1)\n",
        "            wiki=wiki.view(1,wiki.size()[0],-1)\n",
        "            wiki=wiki.expand(nvec.size()[0],wiki.size()[1],-1)\n",
        "            new=wiki*nvec\n",
        "            new=self.embedding(new)\n",
        "            vattention=self.sigmoid(self.vattention(new))\n",
        "            new=new*vattention\n",
        "            vec3=self.layer2(new)\n",
        "            vec3=vec3.view(batchsize,-1)\n",
        "        \n",
        "       \n",
        "        vec2 = self.hidden2tag(output4)\n",
        "        if simlearning==1:\n",
        "            tag_scores = self.sigmoid(vec2.detach()+vec3)\n",
        "        else:\n",
        "            tag_scores = self.sigmoid(vec2)\n",
        "        \n",
        "        \n",
        "        return tag_scores\n",
        "\n",
        "######################################################################\n",
        "# Train the model:\n",
        "\n",
        "topk=10\n",
        "\n",
        "def trainmodel(model, sim):\n",
        "    print ('start_training')\n",
        "    modelsaved=[]\n",
        "    modelperform=[]\n",
        "    topk=10\n",
        "    \n",
        "    \n",
        "    bestresults=-1\n",
        "    bestiter=-1\n",
        "    for epoch in range(5000):  \n",
        "        model.train()\n",
        "        \n",
        "        lossestrain = []\n",
        "        recall=[]\n",
        "        for mysentence in batchtraining_data:\n",
        "            model.zero_grad()\n",
        "            \n",
        "            targets = mysentence[2].cuda()\n",
        "            tag_scores = model(mysentence[0].cuda(),mysentence[1].cuda(),wikivec.cuda(),sim)\n",
        "            loss = loss_function(tag_scores, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lossestrain.append(loss.data.mean())\n",
        "        print (epoch)\n",
        "        modelsaved.append(copy.deepcopy(model.state_dict()))\n",
        "        print (\"XXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
        "        model.eval()\n",
        "    \n",
        "        recall=[]\n",
        "        for inputs in batchval_data:\n",
        "           \n",
        "            targets = inputs[2].cuda()\n",
        "            tag_scores = model(inputs[0].cuda(),inputs[1].cuda() ,wikivec.cuda(),sim)\n",
        "    \n",
        "            loss = loss_function(tag_scores, targets)\n",
        "            \n",
        "            targets=targets.data.cpu().numpy()\n",
        "            tag_scores= tag_scores.data.cpu().numpy()\n",
        "            \n",
        "            \n",
        "            for iii in range(0,len(tag_scores)):\n",
        "                temp={}\n",
        "                for iiii in range(0,len(tag_scores[iii])):\n",
        "                    temp[iiii]=tag_scores[iii][iiii]\n",
        "                temp1=[(k, temp[k]) for k in sorted(temp, key=temp.get, reverse=True)]\n",
        "                thistop=int(np.sum(targets[iii]))\n",
        "                hit=0.0\n",
        "                for ii in temp1[0:max(thistop,topk)]:\n",
        "                    if targets[iii][ii[0]]==1.0:\n",
        "                        hit=hit+1\n",
        "                if thistop!=0:\n",
        "                    recall.append(hit/thistop)\n",
        "            \n",
        "        print ('validation top-',topk, np.mean(recall))\n",
        "        \n",
        "        \n",
        "        \n",
        "        modelperform.append(np.mean(recall))\n",
        "        if modelperform[-1]>bestresults:\n",
        "            bestresults=modelperform[-1]\n",
        "            bestiter=len(modelperform)-1\n",
        "        \n",
        "        if (len(modelperform)-bestiter)>5:\n",
        "            print (modelperform,bestiter)\n",
        "            return modelsaved[bestiter]\n",
        "    \n",
        "model = CNN(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "model.cuda()\n",
        "\n",
        "tic = time.perf_counter()\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "basemodel= trainmodel(model, 0)\n",
        "torch.save(basemodel, 'CNN_model_new')\n",
        "\n",
        "model = CNN(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "model.cuda()\n",
        "model.load_state_dict(basemodel)\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "KSImodel= trainmodel(model, 1)\n",
        "torch.save(KSImodel, 'KSI_CNN_model_new')\n",
        "toc = time.perf_counter()\n",
        "print(f'Time taken to train CNN model is {toc - tic:0.4f} seconds')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-V2zLb7FqbAD",
        "outputId": "f1c47bad-987a-43c6-f970-34f13424d863"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start_training\n",
            "0\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.4452868566717913\n",
            "1\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5489003644601724\n",
            "2\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.5885507996939873\n",
            "3\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.618261070049434\n",
            "4\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.6520349742281973\n",
            "5\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.6719915837337994\n",
            "6\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.6900970889593399\n",
            "7\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.6959143958864301\n",
            "8\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7085743029552194\n",
            "9\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7171614064764273\n",
            "10\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7205637723960405\n",
            "11\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7268510016361591\n",
            "12\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.734076688743684\n",
            "13\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7382890935606621\n",
            "14\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.74001931735448\n",
            "15\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7426945270129973\n",
            "16\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.744641831081218\n",
            "17\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7476934907571152\n",
            "18\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7500733016495338\n",
            "19\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7502050704306114\n",
            "20\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7526563848843904\n",
            "21\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7525305843164437\n",
            "22\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7548491392126907\n",
            "23\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7566139286041739\n",
            "24\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7588117943060243\n",
            "25\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7557926210994755\n",
            "26\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7571250701852205\n",
            "27\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7565610276946134\n",
            "28\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7579952613322867\n",
            "29\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7588098587925186\n",
            "[0.4452868566717913, 0.5489003644601724, 0.5885507996939873, 0.618261070049434, 0.6520349742281973, 0.6719915837337994, 0.6900970889593399, 0.6959143958864301, 0.7085743029552194, 0.7171614064764273, 0.7205637723960405, 0.7268510016361591, 0.734076688743684, 0.7382890935606621, 0.74001931735448, 0.7426945270129973, 0.744641831081218, 0.7476934907571152, 0.7500733016495338, 0.7502050704306114, 0.7526563848843904, 0.7525305843164437, 0.7548491392126907, 0.7566139286041739, 0.7588117943060243, 0.7557926210994755, 0.7571250701852205, 0.7565610276946134, 0.7579952613322867, 0.7588098587925186] 24\n",
            "start_training\n",
            "0\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7733758486886138\n",
            "1\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.779705927215656\n",
            "2\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7810381639971343\n",
            "3\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7830150030587243\n",
            "4\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7846712185107173\n",
            "5\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7869552124969694\n",
            "6\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7864360971561979\n",
            "7\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7843738026056473\n",
            "8\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7830258569718654\n",
            "9\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7826793344493284\n",
            "10\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "validation top- 10 0.7820842383355665\n",
            "[0.7733758486886138, 0.779705927215656, 0.7810381639971343, 0.7830150030587243, 0.7846712185107173, 0.7869552124969694, 0.7864360971561979, 0.7843738026056473, 0.7830258569718654, 0.7826793344493284, 0.7820842383355665] 5\n",
            "Time taken to train CNN model is 708.3468 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def testmodel(modelstate, sim):\n",
        "    model = CNN(batchsize, len(word_to_ix), len(label_to_ix))\n",
        "    model.cuda()\n",
        "    model.load_state_dict(modelstate)\n",
        "    loss_function = nn.BCELoss()\n",
        "    model.eval()\n",
        "    recall=[]\n",
        "    lossestest = []\n",
        "    \n",
        "    y_true=[]\n",
        "    y_scores=[]\n",
        "    \n",
        "    \n",
        "    for inputs in batchtest_data:\n",
        "       \n",
        "        targets = inputs[2].cuda()\n",
        "        \n",
        "        tag_scores = model(inputs[0].cuda(),inputs[1].cuda() ,wikivec.cuda(),sim)\n",
        "\n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        \n",
        "        targets=targets.data.cpu().numpy()\n",
        "        tag_scores= tag_scores.data.cpu().numpy()\n",
        "        \n",
        "        \n",
        "        lossestest.append(loss.data.cpu().numpy().mean())\n",
        "        y_true.append(targets)\n",
        "        y_scores.append(tag_scores)\n",
        "        \n",
        "        for iii in range(0,len(tag_scores)):\n",
        "            temp={}\n",
        "            for iiii in range(0,len(tag_scores[iii])):\n",
        "                temp[iiii]=tag_scores[iii][iiii]\n",
        "            temp1=[(k, temp[k]) for k in sorted(temp, key=temp.get, reverse=True)]\n",
        "            thistop=int(np.sum(targets[iii]))\n",
        "            hit=0.0\n",
        "            \n",
        "            for ii in temp1[0:max(thistop,topk)]:\n",
        "                if targets[iii][ii[0]]==1.0:\n",
        "                    hit=hit+1\n",
        "            if thistop!=0:\n",
        "                recall.append(hit/thistop)\n",
        "    y_true=np.concatenate(y_true,axis=0)\n",
        "    y_scores=np.concatenate(y_scores,axis=0)\n",
        "    y_true=y_true.T\n",
        "    y_scores=y_scores.T\n",
        "    temptrue=[]\n",
        "    tempscores=[]\n",
        "    for  col in range(0,len(y_true)):\n",
        "        if np.sum(y_true[col])!=0:\n",
        "            temptrue.append(y_true[col])\n",
        "            tempscores.append(y_scores[col])\n",
        "    temptrue=np.array(temptrue)\n",
        "    tempscores=np.array(tempscores)\n",
        "    y_true=temptrue.T\n",
        "    y_scores=tempscores.T\n",
        "    y_pred=(y_scores>0.5).astype(np.int)\n",
        "    print ('test loss', np.mean(lossestest))\n",
        "    print ('top-',topk, np.mean(recall))\n",
        "    print ('macro AUC', roc_auc_score(y_true, y_scores,average='macro'))\n",
        "    print ('micro AUC', roc_auc_score(y_true, y_scores,average='micro'))\n",
        "    print ('macro F1', f1_score(y_true, y_pred, average='macro')  )\n",
        "    print ('micro F1', f1_score(y_true, y_pred, average='micro')  )\n",
        "\n",
        "print ('CNN alone:           ')\n",
        "testmodel(basemodel, 0)\n",
        "print ('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
        "print ('KSI+CNN:           ')\n",
        "testmodel(KSImodel, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwmPMDgYqeJ_",
        "outputId": "64b5c55b-3339-4ad9-8331-95c025d660f2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN alone:           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-36bd227b5baa>:57: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y_pred=(y_scores>0.5).astype(np.int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss 0.03858439\n",
            "top- 10 0.7490950576982691\n",
            "macro AUC 0.8343348191076977\n",
            "micro AUC 0.9664729290591237\n",
            "macro F1 0.20455380653471333\n",
            "micro F1 0.6258538131117526\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "KSI+CNN:           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-36bd227b5baa>:57: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y_pred=(y_scores>0.5).astype(np.int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss 0.03622847\n",
            "top- 10 0.7816185284641\n",
            "macro AUC 0.8727390365821882\n",
            "micro AUC 0.9740119253232242\n",
            "macro F1 0.2519865736970157\n",
            "micro F1 0.6434497664365892\n"
          ]
        }
      ]
    }
  ]
}